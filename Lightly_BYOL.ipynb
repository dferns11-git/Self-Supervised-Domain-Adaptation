{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "653e148e-11b4-487d-9f8f-00602b1ef89f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/semiclass/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import copy\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import lightly\n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.data import SimCLRCollateFunction\n",
    "from lightly.loss import NegativeCosineSimilarity\n",
    "from lightly.utils import BenchmarkModule\n",
    "from lightly.models.modules import BYOLProjectionHead, BYOLPredictionHead\n",
    "from lightly.models.utils import deactivate_requires_grad\n",
    "from lightly.models.utils import update_momentum\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "src = \"/lables\"\n",
    "dataset_csv = \"dataset_csv\"\n",
    "\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b0f889-7402-4e5a-b58b-aa5cba279499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Settings for Caltech 256 dataset\n",
    "img_size = 150\n",
    "batch_size = 256\n",
    "num_workers = 8\n",
    "max_epochs = 400\n",
    "num_classes = 31\n",
    "\n",
    "lr_factor = batch_size / 128 #Â scales the learning rate linearly with batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75702e8c-64bc-46ee-9eae-07beb2b08553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting up the BYOL class\n",
    "\n",
    "\n",
    "class BYOL(BenchmarkModule):\n",
    "    def __init__(self, backbone, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head        \n",
    "        self.backbone = nn.Sequential(*list(backbone.children())[:-1])\n",
    "        \n",
    "        # # resnet50, w_resnet\n",
    "        # The one the that worked for CIFAR-10 : 2048,4096, 1024, followed by 1024, 4096, 1024\n",
    "        self.projection_head = BYOLProjectionHead(2048, 4096, 256)\n",
    "        self.prediction_head = BYOLPredictionHead(256, 4096, 256)\n",
    "\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        deactivate_requires_grad(self.backbone_momentum)\n",
    "        deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        self.criterion = lightly.loss.NegativeCosineSimilarity()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(y)\n",
    "        p = self.prediction_head(z)\n",
    "        return p\n",
    "\n",
    "    def forward_momentum(self, x):\n",
    "        y = self.backbone_momentum(x).flatten(start_dim=1)\n",
    "        z = self.projection_head_momentum(y)\n",
    "        z = z.detach()\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        update_momentum(self.backbone, self.backbone_momentum, m=0.99)\n",
    "        update_momentum(self.projection_head, self.projection_head_momentum, m=0.99)\n",
    "        (x0, x1), _, _ = batch\n",
    "        p0 = self.forward(x0)\n",
    "        z0 = self.forward_momentum(x0)\n",
    "        p1 = self.forward(x1)\n",
    "        z1 = self.forward_momentum(x1)\n",
    "        loss = 0.5 * (self.criterion(p0, z1) + self.criterion(p1, z0))\n",
    "        self.log('train_loss_ssl', loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        params = list(self.backbone.parameters()) \\\n",
    "            + list(self.projection_head.parameters()) \\\n",
    "            + list(self.prediction_head.parameters())\n",
    "        optim = torch.optim.SGD(\n",
    "            params, \n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9, \n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "    \n",
    "    def training_epoch_end(self, outputs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2efd8d56-0bcc-4887-8518-da0dfc390455",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "collate_fn = SimCLRCollateFunction(\n",
    "                 input_size=img_size, \n",
    "                 gaussian_blur=0.,\n",
    "                 hf_prob=0.5,\n",
    "                 vf_prob=0.5,\n",
    "                 rr_prob=0.5,\n",
    "                 cj_prob=0.0,\n",
    "                 random_gray_scale=0.0\n",
    "             )\n",
    "\n",
    "class OfficeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_csv_filepath, label_csv_filepath, transform=None):\n",
    "        self.img_csv_file = pd.read_csv(img_csv_filepath)\n",
    "        self.label_csv_file = pd.read_csv(label_csv_filepath)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_csv_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_det = self.img_csv_file.iloc[idx]\n",
    "        image = Image.open(file_det[\"path\"])\n",
    "        label = file_det[\"label\"]\n",
    "        label = self.label_csv_file[self.label_csv_file[\"label\"] == label].values[0][1]\n",
    "        # label = torch.tensor(label)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return (image, label, file_det[\"filename\"])\n",
    "\n",
    "\n",
    "# label_csv = join(src, dataset_csv, \"office_31_labels.csv\")\n",
    "label_csv = join(src, dataset_csv, \"caltech_256_labels.csv\")\n",
    "\n",
    "train_csv = join(src, dataset_csv, \"paperspace_caltech_256_train.csv\")\n",
    "train_dataset = OfficeDataset(img_csv_filepath = train_csv,\n",
    "                              label_csv_filepath = label_csv)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              collate_fn = collate_fn,\n",
    "                              drop_last=True,\n",
    "                              num_workers=num_workers)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "test_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(img_size),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=lightly.data.collate.imagenet_normalize['mean'],\n",
    "        std=lightly.data.collate.imagenet_normalize['std'],\n",
    "    )\n",
    "])\n",
    "\n",
    "classif_train_dataset = OfficeDataset(img_csv_filepath = train_csv,\n",
    "                              label_csv_filepath = label_csv,\n",
    "                              transform = test_transforms)\n",
    "\n",
    "# classif_train_dataset = LightlyDataset(\"office_31/dslr\",\n",
    "#                                        transform = test_transforms)\n",
    "\n",
    "# classif_train_dataset =  torchvision.datasets.ImageFolder(\"modern_office_31/amazon\",\n",
    "#                                                            transform = classif_train_transforms)\n",
    "\n",
    "classif_train_dataloader = DataLoader(classif_train_dataset, \n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=num_workers)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "\n",
    "\n",
    "# Initialize val dataset and dataloader\n",
    "# test_csv = join(src, dataset_csv, \"paperspace_office_31_webcam.csv\")\n",
    "# test_csv = join(src, dataset_csv, \"paperspace_modern_office_31_new_amazon.csv\")\n",
    "test_csv = join(src, dataset_csv, \"paperspace_caltech_256_test.csv\")\n",
    "test_dataset = OfficeDataset(img_csv_filepath = test_csv,\n",
    "                              label_csv_filepath = label_csv,\n",
    "                              transform = test_transforms)\n",
    "\n",
    "# test_dataset = LightlyDataset(\"office_31/amazon\",\n",
    "#                               transform = test_transforms)\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=False,\n",
    "                             num_workers=num_workers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f8a7bfc",
   "metadata": {},
   "source": [
    "### Load the encoder weights from pre-training before trainin in BYOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45da5ee9-1ec1-46a3-8ae5-21f395933731",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model = torchvision.models.resnet50()\n",
    "model_path = \"models/resnet50-19c8e357.pth\"\n",
    "\n",
    "\n",
    "base_model.load_state_dict(torch.load(join(src,model_path)), strict=False) \n",
    "model = BYOL(base_model, train_dataloader, num_classes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97caea21",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79f48548-8e9d-4949-bf9f-81e2e2ef7adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs, \n",
    "    gpus=1)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=train_dataloader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "207490af",
   "metadata": {},
   "source": [
    "### Save the encoder in .pth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac909249-ec6a-4bc4-8859-460f0cdc2fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_backbone = model.backbone\n",
    "\n",
    "# you can also store the backbone and use it in another code\n",
    "state_dict = {\n",
    "    'model_params': pretrained_model_backbone.state_dict(),\n",
    "    'epochs' : 800,\n",
    "    'output_dim' : 64,\n",
    "    'batch_size' : 256,\n",
    "    'img_size' : 150,\n",
    "    'color_augs' : False\n",
    "}\n",
    "torch.save(state_dict, 'res50_byol_modern_office31_amazon_v1.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3cac9b3d",
   "metadata": {},
   "source": [
    "### Load the model from .pth file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c18c9f2f-f010-4145-bb47-457dae0d96d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "2048\n",
      "256\n",
      "150\n",
      "True\n",
      "0.3\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# load the model in a new file for inference\n",
    "# model_new = torchvision.models.resnet34()\n",
    "model_new = torchvision.models.resnet50()\n",
    "\n",
    "backbone_new = nn.Sequential(*list(model_new.children())[:-1])\n",
    "\n",
    "ckpt = torch.load('res50_dino_caltech.pth')\n",
    "backbone_new.load_state_dict(ckpt['model_params'])\n",
    "print(ckpt['epochs'])\n",
    "print(ckpt['output_dim'])\n",
    "print(ckpt['batch_size'])\n",
    "print(ckpt['img_size'])\n",
    "print(ckpt['color_augs'])\n",
    "print(ckpt['lr'])\n",
    "print(ckpt['random_init'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ad81093",
   "metadata": {},
   "source": [
    "## Use a KNN classifer to test the performance of the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89c3000b-6424-400e-92b1-1e1c075a0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(model, dataloader):\n",
    "    \"\"\"Generates representations for all images in the dataloader with\n",
    "    the given model\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    filenames = []\n",
    "    with torch.no_grad():\n",
    "        for img, label, fnames in dataloader:\n",
    "            img = img.to(\"cpu\")\n",
    "            emb = model(img).flatten(start_dim=1)\n",
    "            embeddings.append(emb)\n",
    "            labels.extend(label)\n",
    "            filenames.extend(fnames)\n",
    "\n",
    "    embeddings = torch.cat(embeddings, 0)\n",
    "    embeddings = normalize(embeddings)\n",
    "    return torch.tensor(embeddings), torch.tensor(labels), filenames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "336d7398-854b-4690-8093-8f9ccaa4e4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traing embeddings loaded\n"
     ]
    }
   ],
   "source": [
    "backbone_new.eval()\n",
    "train_embeddings, train_labels, train_filenames = generate_embeddings(backbone_new, classif_train_dataloader)\n",
    "print(\"Traing embeddings loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df1b24c8-aec2-4bb3-99c0-1a6e87e7d572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test embeddings done\n"
     ]
    }
   ],
   "source": [
    "test_embeddings, test_labels, test_filenames = generate_embeddings(backbone_new, test_dataloader)\n",
    "print(\"Test embeddings done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ee6caad-54ff-4a5b-96d9-470bdfcfc64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "795"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cea48dfd-0e98-4a0e-b64e-d8a3e6af0c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2817"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e27eb630-1c18-4a22-bacb-f90885670351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "n_neighbors = 10\n",
    "knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "knn.fit(train_embeddings.numpy(), train_labels.numpy())\n",
    "y_pred = knn.predict(test_embeddings.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a219d4ec-5f9f-4c70-b187-16e7712add8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.677734375"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fca77170-efaa-4d32-8006-f73dcec734e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2066102b-d8da-4405-a41b-6cf4460902fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1845d8ef-f42f-4292-a577-6b665ae79890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6859375"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightly.utils import knn_predict\n",
    "import torch.nn.functional as F\n",
    "pred = knn_predict(test_embeddings, train_embeddings.T, train_labels.unsqueeze(dim=0), 256,20)\n",
    "accuracy_score(pred[:,0], test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92753023-f23c-4bed-9f77-137d1d18ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_acc, clusters = 0, 0\n",
    "\n",
    "for r in range(1, 200):\n",
    "    pred = knn_predict(test_embeddings, train_embeddings.T, train_labels.unsqueeze(dim=0), 256,r)\n",
    "    acc = accuracy_score(pred[:,0], test_labels)\n",
    "    if acc > max_acc:\n",
    "        max_acc = acc\n",
    "        clusters = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0cb3e07-c2c4-42e0-80fd-0c96655fee71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6890625"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7563ff3-408d-43ca-9279-21f0a165a796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
